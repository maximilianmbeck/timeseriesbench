### HYDRA START
hydra:
  run:
    dir: outputs/${hydra.job.name}
  sweep:
    dir: outputs/${hydra.job.name}
    subdir: ${hydra.job.num}
  job:
    chdir: True # change working directory of run
    name: ${config.experiment_data.experiment_name}_${now:%y%m%d_%H%M%S}
### HYDRA END

### RUN CONFIG START
run_config:
  exec_type: parallel # sequential
  hostname: null # < filled by run_handler
  gpu_ids: 0 #[0, 1]
  runs_per_gpu: 3

  # wandb: # wandb config for run_handler, if "wandb: null" then logging to wandb is disabled for run_handler
  #   init:
  #     tags:
  #       - ${config.experiment_data.experiment_tag}_exps
  #       - run_handler
  #     notes: #
  #     group: ${config.experiment_data.experiment_tag}
  #     job_type: run_handler

#! Seed option 1:
seeds: # run every run with every given seed
  - 1
  # - 2
#! Seed option 2:
# seeds: null # allows to override seeds via sweep, uses predefined seed in experiment_data

#! Grid sweep:
# sweep:
#   type: grid # create runs for every possible parameter combination
#   axes:
#     - parameter: trainer.x # supports sweeping over whole subconfigs
#       vals:
#         - [32, 64]
#         - [64, 256]
#     - parameter: trainer.optimizer_args.lr # standard sweep configuration
#       vals:
#         - 0.001
#         - 0.01
#         - 0.1
#     - parameter: [trainer.optimizer_args.a, trainer.batch_size] # these two parameters are varied together
#       vals:
#         - 0.001 # a
#         - 64 # batch_size
#         - 0.1 # a
#         - 128 # batch_size
#     - parameter: data.rotation
#       vals: arange_int(0,10,1) # arange_float(0,20,2.5) # linspace(0,30,7.5, endpoint=True)

#! Line sweep:
# sweep:
#   type: line # create runs specified as lists for every parameter. Make sure that every val list has the same length.
#   axes:
#     - parameter: trainer.x # supports sweeping over whole subconfigs
#       vals:
#         - [32, 64] # goes together with lr=0.001
#         - [64, 256] # goes together with lr=0.01
#     - parameter: trainer.optimizer_args.lr # standard sweep configuration
#       vals:
#         - 0.001
#         - 0.01

#! Random grid sweep: (same as 'grid', but also specify how many runs you want to sample, i.e. 'num_runs')
sweep:
  type: random_grid # create runs for every possible parameter combination
  num_runs: 4
  axes:
    - parameter: trainer.x # supports sweeping over whole subconfigs
      vals:
        - [32, 64]
        - [64, 256]
    - parameter: trainer.optimizer_args.lr # standard sweep configuration
      vals:
        - 0.001
        - 0.01
        - 0.1
    - parameter: data.rotation
      vals: arange_int(0,10,1) # arange_float(0,20,2.5) # linspace(0,30,7.5, endpoint=True)

#! Random sweep
# sweep:
#   type: random
#   num_runs: 5
#   axes:
#     - parameter: trainer.optimizer_args.lr
#       searchspace: uniform
#       vals: [0.001, 1.0] # lower value is used for lower bound
#     - parameter: trainer.batch_size
#       searchspace: uniformint
#       vals: [64, 256, 64] # lower, upper, quantization (or: step)
#     - parameter: trainer.x
#       searchspace: choice
#       vals:
#         - [32, 64]
#         - [64, 256]

#? Possible searchspaces:
# uniform, quniform, loguniform, qloguniform, normal, qnormal, uniformint, loguniformint, qloguniformint, choice
# see https://docs.ray.io/en/latest/tune/api_docs/search_space.html#search-space-api

### RUN CONFIG END

###################################

config:
  ### EXPERIMENT CONFIG
  experiment_data:
    entity: null # jkuiml-fsl # mbeck
    project_name: example_project
    experiment_tag: "0.0" # a unique ID for the experiment
    experiment_name: sweep-${config.experiment_data.experiment_tag} # use parameter references
    experiment_dir: null # will be filled when run starts
    seed: 42
    hostname: null # the server on which the run is run, will be filled by run_handler
    gpu_id: 0
  ###

  wandb: # wandb config for single runs
    init: # passed to wandb.init()
      tags: # list(), used to tag wandblogger
        - DEBUG
      notes: Trying different things. # str, used to make notes to wandblogger
      group: ${config.experiment_data.experiment_tag} # null # str, organize individual runs into larger experiments
      job_type: pretrain # examples: hypsearch, pretrain, eval, etc. # str, specify the type of the runs, which is useful when grouping runs together

    watch: # passed to wandb.watch()
      log: all
      log_freq: 100

  trainer:
    x: [10, 10]
    optimizer_args:
      lr: 1.0
      a: 9
    batch_size: 256

  model:
    name: fc
    hidden_size: 512 # add additional parameter (never accessed in application, only in config)
    model_kwargs:
      input_size: 784
      hidden_sizes:
        - ${config.model.hidden_size} # can be used for easy sweeping
        - ${config.model.hidden_size}
      output_size: 10
      flatten_input: true
      dropout: 0.2
      act_fn: relu

  data:
    rotation: 0
