{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/maximilian.beck/.conda/envs/xlstmdev1/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "from omegaconf import OmegaConf\n",
    "from dacite import from_dict\n",
    "\n",
    "from tsbench.tslib.loading.csv_loader import CSVTimeSeriesDataset\n",
    "from tsbench.tslib.traindataset_generator import TimeSeriesTrainDatasetGeneratorConfig, TimeSeriesTrainDatasetGenerator\n",
    "\n",
    "from tsbench.tslib.utils import benchmark_dataloading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark dataloading with tslib\n",
    "\n",
    "In initial experiments we observed, very slow dataloading especially when the normalizer was enabled.\n",
    "In this notebook we investigate this further and hopefully find a solution (e.g. caching the final dataset in memory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FILE = '../../datafiles/har_with_smartphones/train.csv'\n",
    "TRAIN_FILE = Path(TRAIN_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark 1: raw dataset loading\n",
    "raw_ds = CSVTimeSeriesDataset(data_file=TRAIN_FILE, meta_columns=['subject', 'Activity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 126/126 [00:00<00:00, 516.69it/s]\n",
      "Sample: 100%|██████████| 126/126 [00:00<00:00, 519.21it/s]\n",
      "Generating window index: 100%|██████████| 126/126 [00:00<00:00, 11070.01it/s]\n",
      "Total number of dropped timesteps due to windowing: 272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/iarai/home/maximilian.beck/repos/tsbench/notebooks/dev/../../tsbench/tslib/postprocessing/dataset_subset.py:152: UserWarning: Length of split at index 1 is 0. This might result in an empty dataset.\n",
      "  warnings.warn(f\"Length of split at index {i} is 0. \" f\"This might result in an empty dataset.\")\n"
     ]
    }
   ],
   "source": [
    "# Benchmark 2: full train dataset without normalization\n",
    "cfg = \"\"\" \n",
    "pipeline:\n",
    "  dataset:\n",
    "    name: csvloader\n",
    "    kwargs:\n",
    "      data_file: /iarai/home/maximilian.beck/repos/tsbench_dev/datafiles/har_with_smartphones/train.csv\n",
    "      meta_columns: [subject, Activity]\n",
    "  windowing:\n",
    "    window_size: 20 # each time series for the model will have length 10\n",
    "    stride: 5 # each time series will be shifted by 5\n",
    "  # normalizer: #! Note: this slows down training by a factor of 10!! Must be fixed!\n",
    "  #   normalizer_file: /iarai/home/maximilian.beck/repos/tsbench/datafiles/har_with_smartphones/normalizer.json\n",
    "  target_generator:\n",
    "    name: csv_classification\n",
    "    kwargs:\n",
    "      class_column: Activity\n",
    "      class_labels: ['STANDING', 'SITTING', 'LAYING', 'WALKING', 'WALKING_DOWNSTAIRS', 'WALKING_UPSTAIRS']\n",
    "split: \n",
    "  name: random_split\n",
    "  kwargs:\n",
    "    lengths: [1.0, 0.0] # train, val\n",
    "\"\"\"\n",
    "cfg = OmegaConf.create(cfg)\n",
    "\n",
    "cfg = from_dict(data_class=TimeSeriesTrainDatasetGeneratorConfig, data=OmegaConf.to_container(cfg))\n",
    "\n",
    "train_ds_no_norm = TimeSeriesTrainDatasetGenerator(cfg)\n",
    "train_ds_no_norm.generate_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 126/126 [00:00<00:00, 505.47it/s]\n",
      "Sample: 100%|██████████| 126/126 [00:00<00:00, 510.19it/s]\n",
      "Generating window index: 100%|██████████| 126/126 [00:00<00:00, 11645.97it/s]\n",
      "Total number of dropped timesteps due to windowing: 272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/iarai/home/maximilian.beck/repos/tsbench/notebooks/dev/../../tsbench/tslib/postprocessing/dataset_subset.py:152: UserWarning: Length of split at index 1 is 0. This might result in an empty dataset.\n",
      "  warnings.warn(f\"Length of split at index {i} is 0. \" f\"This might result in an empty dataset.\")\n"
     ]
    }
   ],
   "source": [
    "# Benchmark 3: full train dataset with normalization\n",
    "cfg = \"\"\" \n",
    "pipeline:\n",
    "  dataset:\n",
    "    name: csvloader\n",
    "    kwargs:\n",
    "      data_file: /iarai/home/maximilian.beck/repos/tsbench_dev/datafiles/har_with_smartphones/train.csv\n",
    "      meta_columns: [subject, Activity]\n",
    "  windowing:\n",
    "    window_size: 20 # each time series for the model will have length 10\n",
    "    stride: 5 # each time series will be shifted by 5\n",
    "  normalizer: #! Note: this slows down training by a factor of 10!! Must be fixed!\n",
    "    normalizer_file: /iarai/home/maximilian.beck/repos/tsbench/datafiles/har_with_smartphones/normalizer.json\n",
    "  target_generator:\n",
    "    name: csv_classification\n",
    "    kwargs:\n",
    "      class_column: Activity\n",
    "      class_labels: ['STANDING', 'SITTING', 'LAYING', 'WALKING', 'WALKING_DOWNSTAIRS', 'WALKING_UPSTAIRS']\n",
    "split: \n",
    "  name: random_split\n",
    "  kwargs:\n",
    "    lengths: [1.0, 0.0] # train, val\n",
    "\"\"\"\n",
    "cfg = OmegaConf.create(cfg)\n",
    "\n",
    "cfg = from_dict(data_class=TimeSeriesTrainDatasetGeneratorConfig, data=OmegaConf.to_container(cfg))\n",
    "\n",
    "train_ds_norm = TimeSeriesTrainDatasetGenerator(cfg)\n",
    "train_ds_norm.generate_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1038"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds_norm.train_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this benchmark we see, that normalization of each timeseries upon loading as implemented in the baseline slows down loading by a factor > 20!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Benchmark 1: raw dataset loading\n",
      "Sample: 100%|██████████| 126/126 [00:00<00:00, 620.02it/s]\n",
      "Sample: 100%|██████████| 126/126 [00:00<00:00, 646.11it/s]\n",
      "Epoch: 100%|██████████| 2/2 [00:00<00:00,  4.91it/s]\n",
      "Dataloading benchmark for CSVTimeSeriesDataset\n",
      "Time in seconds\n",
      "Time per epoch (num_epochs=2): [0.20908284187316895, 0.20009446144104004]\n",
      "Average time per epoch: 0.2045886516571045\n",
      "Total time: 0.41030216217041016\n",
      "Time first epoch: 0.20908284187316895 / Time last epoch: 0.20009446144104004\n",
      "\n",
      "Benchmark 2: full train dataset without normalization\n",
      "Sample: 100%|██████████| 1038/1038 [00:04<00:00, 248.97it/s]\n",
      "Sample: 100%|██████████| 1038/1038 [00:04<00:00, 251.58it/s]\n",
      "Epoch: 100%|██████████| 2/2 [00:08<00:00,  4.15s/it]\n",
      "Dataloading benchmark for TimeSeriesTrainDatasetSubset\n",
      "Time in seconds\n",
      "Time per epoch (num_epochs=2): [4.174413442611694, 4.129810094833374]\n",
      "Average time per epoch: 4.152111768722534\n",
      "Total time: 8.305387020111084\n",
      "Time first epoch: 4.174413442611694 / Time last epoch: 4.129810094833374\n",
      "\n",
      "Benchmark 3: full train dataset with normalization\n",
      "Sample: 100%|██████████| 1038/1038 [01:34<00:00, 11.00it/s]\n",
      "Sample: 100%|██████████| 1038/1038 [01:33<00:00, 11.11it/s]\n",
      "Epoch: 100%|██████████| 2/2 [03:07<00:00, 93.89s/it]\n",
      "Dataloading benchmark for TimeSeriesTrainDatasetSubset\n",
      "Time in seconds\n",
      "Time per epoch (num_epochs=2): [94.37831091880798, 93.40296387672424]\n",
      "Average time per epoch: 93.89063739776611\n",
      "Total time: 187.78234481811523\n",
      "Time first epoch: 94.37831091880798 / Time last epoch: 93.40296387672424\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 2\n",
    "print('\\nBenchmark 1: raw dataset loading')\n",
    "benchmark_dataloading(raw_ds, NUM_EPOCHS)\n",
    "\n",
    "print('\\nBenchmark 2: full train dataset without normalization')\n",
    "benchmark_dataloading(train_ds_no_norm.train_split, NUM_EPOCHS)\n",
    "\n",
    "print('\\nBenchmark 3: full train dataset with normalization')\n",
    "benchmark_dataloading(train_ds_norm.train_split, NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tsbench.tslib.target.target_dataset.TimeSeriesTargetDataset at 0x7ff5380e65d0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds_norm.train_split.dataset.dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution Approach: Cache the final pipeline result of the in the TimeSeriesTrainDataset and load the full dataset upon creation once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy Array vs dictionary for caching\n",
    "https://chat.openai.com/share/72e8de07-ac42-4f87-9ea2-b4a4effca906\n",
    "\n",
    "conclusion: we use numpy arrays as the access for the target dataset is always via integers indices and we want to save memory and optimize for speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy array memory usage: 8000112 bytes\n",
      "Dictionary memory usage: 41943128 bytes\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "# Example dataset size\n",
    "dataset_size = 1000000\n",
    "\n",
    "# NumPy array with integers\n",
    "numpy_array = np.zeros(dataset_size, dtype=object)\n",
    "\n",
    "# Dictionary with integers\n",
    "dictionary = {i: 0 for i in range(dataset_size)}\n",
    "\n",
    "# Memory usage in bytes\n",
    "numpy_memory_usage = sys.getsizeof(numpy_array)\n",
    "dictionary_memory_usage = sys.getsizeof(dictionary)\n",
    "\n",
    "print(f\"NumPy array memory usage: {numpy_memory_usage} bytes\")\n",
    "print(f\"Dictionary memory usage: {dictionary_memory_usage} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy array access time: 0.002144481986761093 seconds\n",
      "Dictionary access time: 0.0022666417062282562 seconds\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import timeit\n",
    "\n",
    "# Example dataset size\n",
    "dataset_size = 1000000\n",
    "\n",
    "# Creating a NumPy array with integers\n",
    "numpy_array = np.arange(dataset_size)\n",
    "\n",
    "# Creating a dictionary with integers\n",
    "dictionary = {i: i for i in range(dataset_size)}\n",
    "\n",
    "# Function to measure access time for NumPy array\n",
    "def access_numpy_array():\n",
    "    index = np.random.randint(0, dataset_size)\n",
    "    value = numpy_array[index]\n",
    "\n",
    "# Function to measure access time for dictionary\n",
    "def access_dictionary():\n",
    "    index = np.random.randint(0, dataset_size)\n",
    "    value = dictionary[index]\n",
    "\n",
    "# Measure access time for NumPy array\n",
    "numpy_access_time = timeit.timeit(access_numpy_array, number=1000)\n",
    "\n",
    "# Measure access time for dictionary\n",
    "dictionary_access_time = timeit.timeit(access_dictionary, number=1000)\n",
    "\n",
    "print(f\"NumPy array access time: {numpy_access_time} seconds\")\n",
    "print(f\"Dictionary access time: {dictionary_access_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results: Added Caching to TargetDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 126/126 [00:00<00:00, 504.40it/s]\n",
      "Sample: 100%|██████████| 126/126 [00:00<00:00, 506.60it/s]\n",
      "Generating window index: 100%|██████████| 126/126 [00:00<00:00, 11659.33it/s]\n",
      "Total number of dropped timesteps due to windowing: 272\n",
      "Fill Processed Items Cache: 100%|██████████| 1038/1038 [00:04<00:00, 256.53it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/iarai/home/maximilian.beck/repos/tsbench/notebooks/dev/../../tsbench/tslib/postprocessing/dataset_subset.py:152: UserWarning: Length of split at index 1 is 0. This might result in an empty dataset.\n",
      "  warnings.warn(f\"Length of split at index {i} is 0. \" f\"This might result in an empty dataset.\")\n"
     ]
    }
   ],
   "source": [
    "# Benchmark 2: full train dataset without normalization\n",
    "cfg = \"\"\" \n",
    "pipeline:\n",
    "  dataset:\n",
    "    name: csvloader\n",
    "    kwargs:\n",
    "      data_file: /iarai/home/maximilian.beck/repos/tsbench_dev/datafiles/har_with_smartphones/train.csv\n",
    "      meta_columns: [subject, Activity]\n",
    "  windowing:\n",
    "    window_size: 20 # each time series for the model will have length 10\n",
    "    stride: 5 # each time series will be shifted by 5\n",
    "  # normalizer: #! Note: this slows down training by a factor of 10!! Must be fixed!\n",
    "  #   normalizer_file: /iarai/home/maximilian.beck/repos/tsbench/datafiles/har_with_smartphones/normalizer.json\n",
    "  target_generator:\n",
    "    name: csv_classification\n",
    "    kwargs:\n",
    "      class_column: Activity\n",
    "      class_labels: ['STANDING', 'SITTING', 'LAYING', 'WALKING', 'WALKING_DOWNSTAIRS', 'WALKING_UPSTAIRS']\n",
    "  cache_processed_dataset: True #! This is the change!\n",
    "split: \n",
    "  name: random_split\n",
    "  kwargs:\n",
    "    lengths: [1.0, 0.0] # train, val\n",
    "\"\"\"\n",
    "cfg = OmegaConf.create(cfg)\n",
    "\n",
    "cfg = from_dict(data_class=TimeSeriesTrainDatasetGeneratorConfig, data=OmegaConf.to_container(cfg))\n",
    "\n",
    "train_ds_no_norm = TimeSeriesTrainDatasetGenerator(cfg)\n",
    "train_ds_no_norm.generate_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 126/126 [00:00<00:00, 497.51it/s]\n",
      "Sample: 100%|██████████| 126/126 [00:00<00:00, 510.23it/s]\n",
      "Generating window index: 100%|██████████| 126/126 [00:00<00:00, 12564.96it/s]\n",
      "Total number of dropped timesteps due to windowing: 272\n",
      "Fill Processed Items Cache: 100%|██████████| 1038/1038 [01:33<00:00, 11.12it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/iarai/home/maximilian.beck/repos/tsbench/notebooks/dev/../../tsbench/tslib/postprocessing/dataset_subset.py:152: UserWarning: Length of split at index 1 is 0. This might result in an empty dataset.\n",
      "  warnings.warn(f\"Length of split at index {i} is 0. \" f\"This might result in an empty dataset.\")\n"
     ]
    }
   ],
   "source": [
    "# Benchmark 3: full train dataset with normalization\n",
    "cfg = \"\"\" \n",
    "pipeline:\n",
    "  dataset:\n",
    "    name: csvloader\n",
    "    kwargs:\n",
    "      data_file: /iarai/home/maximilian.beck/repos/tsbench_dev/datafiles/har_with_smartphones/train.csv\n",
    "      meta_columns: [subject, Activity]\n",
    "  windowing:\n",
    "    window_size: 20 # each time series for the model will have length 10\n",
    "    stride: 5 # each time series will be shifted by 5\n",
    "  normalizer: #! Note: this slows down training by a factor of 10!! Must be fixed!\n",
    "    normalizer_file: /iarai/home/maximilian.beck/repos/tsbench/datafiles/har_with_smartphones/normalizer.json\n",
    "  target_generator:\n",
    "    name: csv_classification\n",
    "    kwargs:\n",
    "      class_column: Activity\n",
    "      class_labels: ['STANDING', 'SITTING', 'LAYING', 'WALKING', 'WALKING_DOWNSTAIRS', 'WALKING_UPSTAIRS']\n",
    "  cache_processed_dataset: True #! This is the change!\n",
    "\n",
    "split: \n",
    "  name: random_split\n",
    "  kwargs:\n",
    "    lengths: [1.0, 0.0] # train, val\n",
    "\"\"\"\n",
    "cfg = OmegaConf.create(cfg)\n",
    "\n",
    "cfg = from_dict(data_class=TimeSeriesTrainDatasetGeneratorConfig, data=OmegaConf.to_container(cfg))\n",
    "\n",
    "train_ds_norm = TimeSeriesTrainDatasetGenerator(cfg)\n",
    "train_ds_norm.generate_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat Benchmark with caching enabled:\n",
    "Result: Lightning fast dataloading during training and validation, at the cost of iterating just once over the full dataset at the beginning of training. ;-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Benchmark 1: raw dataset loading\n",
      "Epoch:   0%|          | 0/10 [00:00<?, ?it/s]Sample: 100%|██████████| 126/126 [00:00<00:00, 629.32it/s]\n",
      "Sample: 100%|██████████| 126/126 [00:00<00:00, 644.77it/s]\n",
      "Sample: 100%|██████████| 126/126 [00:00<00:00, 645.19it/s]\n",
      "Sample: 100%|██████████| 126/126 [00:00<00:00, 646.75it/s]\n",
      "Sample: 100%|██████████| 126/126 [00:00<00:00, 647.58it/s]\n",
      "Sample: 100%|██████████| 126/126 [00:00<00:00, 661.37it/s]\n",
      "Sample: 100%|██████████| 126/126 [00:00<00:00, 656.72it/s]\n",
      "Sample: 100%|██████████| 126/126 [00:00<00:00, 656.65it/s]\n",
      "Sample: 100%|██████████| 126/126 [00:00<00:00, 655.21it/s]\n",
      "Sample: 100%|██████████| 126/126 [00:00<00:00, 657.21it/s]\n",
      "Epoch: 100%|██████████| 10/10 [00:01<00:00,  5.06it/s]\n",
      "Dataloading benchmark for CSVTimeSeriesDataset\n",
      "Time in seconds\n",
      "Time per epoch (num_epochs=10): [0.20562458038330078, 0.1992647647857666, 0.1990680694580078, 0.19869756698608398, 0.19789576530456543, 0.1941518783569336, 0.19552016258239746, 0.19568467140197754, 0.19582533836364746, 0.19541645050048828]\n",
      "Average time per epoch: 0.1977149248123169\n",
      "Total time: 1.9782137870788574\n",
      "Time first epoch: 0.20562458038330078 / Time last epoch: 0.19541645050048828\n",
      "\n",
      "Benchmark 2: full train dataset without normalization\n",
      "Sample: 100%|██████████| 1038/1038 [00:00<00:00, 1810265.09it/s]\n",
      "Sample: 100%|██████████| 1038/1038 [00:00<00:00, 2050724.24it/s]\n",
      "Sample: 100%|██████████| 1038/1038 [00:00<00:00, 2265186.03it/s]\n",
      "Sample: 100%|██████████| 1038/1038 [00:00<00:00, 2256966.07it/s]\n",
      "Sample: 100%|██████████| 1038/1038 [00:00<00:00, 2289005.02it/s]\n",
      "Sample: 100%|██████████| 1038/1038 [00:00<00:00, 2334416.92it/s]\n",
      "Sample: 100%|██████████| 1038/1038 [00:00<00:00, 2353344.62it/s]\n",
      "Sample: 100%|██████████| 1038/1038 [00:00<00:00, 2341951.35it/s]\n",
      "Sample: 100%|██████████| 1038/1038 [00:00<00:00, 1808760.93it/s]\n",
      "Sample: 100%|██████████| 1038/1038 [00:00<00:00, 1786494.69it/s]\n",
      "Epoch: 100%|██████████| 10/10 [00:00<00:00, 272.21it/s]\n",
      "Dataloading benchmark for TimeSeriesTrainDatasetSubset\n",
      "Time in seconds\n",
      "Time per epoch (num_epochs=10): [0.005231618881225586, 0.003766775131225586, 0.003648042678833008, 0.003178834915161133, 0.004083395004272461, 0.003677845001220703, 0.003658294677734375, 0.003574848175048828, 0.00399017333984375, 0.0038394927978515625]\n",
      "Average time per epoch: 0.0038649320602416994\n",
      "Total time: 0.03918194770812988\n",
      "Time first epoch: 0.005231618881225586 / Time last epoch: 0.0038394927978515625\n",
      "\n",
      "Benchmark 3: full train dataset with normalization\n",
      "Sample: 100%|██████████| 1038/1038 [00:00<00:00, 1528682.43it/s]\n",
      "Sample: 100%|██████████| 1038/1038 [00:00<00:00, 1683560.54it/s]\n",
      "Sample: 100%|██████████| 1038/1038 [00:00<00:00, 1776290.31it/s]\n",
      "Sample: 100%|██████████| 1038/1038 [00:00<00:00, 1796075.72it/s]\n",
      "Sample: 100%|██████████| 1038/1038 [00:00<00:00, 1733846.10it/s]\n",
      "Sample: 100%|██████████| 1038/1038 [00:00<00:00, 1324114.22it/s]\n",
      "Sample: 100%|██████████| 1038/1038 [00:00<00:00, 1299608.22it/s]\n",
      "Sample: 100%|██████████| 1038/1038 [00:00<00:00, 1315313.46it/s]\n",
      "Sample: 100%|██████████| 1038/1038 [00:00<00:00, 1320499.71it/s]\n",
      "Sample: 100%|██████████| 1038/1038 [00:00<00:00, 1326939.21it/s]\n",
      "Epoch: 100%|██████████| 10/10 [00:00<00:00, 248.97it/s]\n",
      "Dataloading benchmark for TimeSeriesTrainDatasetSubset\n",
      "Time in seconds\n",
      "Time per epoch (num_epochs=10): [0.005361080169677734, 0.003726482391357422, 0.004003763198852539, 0.0038802623748779297, 0.0038657188415527344, 0.004289150238037109, 0.0042498111724853516, 0.004235506057739258, 0.004258632659912109, 0.004224300384521484]\n",
      "Average time per epoch: 0.004209470748901367\n",
      "Total time: 0.04267311096191406\n",
      "Time first epoch: 0.005361080169677734 / Time last epoch: 0.004224300384521484\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 10\n",
    "print('\\nBenchmark 1: raw dataset loading')\n",
    "benchmark_dataloading(raw_ds, NUM_EPOCHS)\n",
    "\n",
    "print('\\nBenchmark 2: full train dataset without normalization')\n",
    "benchmark_dataloading(train_ds_no_norm.train_split, NUM_EPOCHS)\n",
    "\n",
    "print('\\nBenchmark 3: full train dataset with normalization')\n",
    "benchmark_dataloading(train_ds_norm.train_split, NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
